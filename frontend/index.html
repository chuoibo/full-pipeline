<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Speech Processing App</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background-color: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }
        h1 {
            color: #333;
            text-align: center;
        }
        .controls {
            display: flex;
            justify-content: center;
            margin: 20px 0;
            gap: 10px;
        }
        button {
            padding: 10px 20px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            font-size: 16px;
            transition: all 0.3s;
        }
        button:hover {
            opacity: 0.9;
        }
        #startBtn {
            background-color: #4CAF50;
            color: white;
        }
        #stopBtn {
            background-color: #f44336;
            color: white;
            display: none;
        }
        .card {
            margin: 20px 0;
            padding: 15px;
            border-radius: 5px;
            border-left: 5px solid #2196F3;
            background-color: #f9f9f9;
        }
        .status {
            text-align: center;
            margin: 10px 0;
            font-weight: bold;
            color: #555;
        }
        #transcription {
            min-height: 100px;
            white-space: pre-wrap;
        }
        #response {
            min-height: 100px;
            max-height: 300px;
            overflow-y: auto;
            white-space: pre-wrap;
        }
        .audio-controls {
            display: flex;
            justify-content: center;
            margin-top: 10px;
        }
        #audioPlayer {
            width: 100%;
            margin-top: 10px;
        }
        .loading {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid rgba(0,0,0,.3);
            border-radius: 50%;
            border-top-color: #2196F3;
            animation: spin 1s ease-in-out infinite;
            margin-left: 10px;
        }
        @keyframes spin {
            to { transform: rotate(360deg); }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Speech Processing App</h1>
        
        <div class="controls">
            <button id="startBtn">Start Speaking</button>
            <button id="stopBtn">Stop</button>
        </div>
        
        <div class="status" id="status">Ready</div>
        
        <div class="card">
            <h3>Your Speech</h3>
            <div id="transcription"></div>
        </div>
        
        <div class="card">
            <h3>Response</h3>
            <div id="response"></div>
            <div class="audio-controls">
                <audio id="audioPlayer" controls></audio>
            </div>
        </div>
    </div>

    <script>
        // DOM elements
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const status = document.getElementById('status');
        const transcription = document.getElementById('transcription');
        const response = document.getElementById('response');
        const audioPlayer = document.getElementById('audioPlayer');
        
        // WebSocket connection for ASR
        let socket;
        let mediaRecorder;
        let audioContext;
        let audioStream;
        let isRecording = false;
        let transcriptionText = "";
        
        // Audio queue for TTS playback
        let audioQueue = [];
        let isPlaying = false;
        
        // Initialize audio
        async function initAudio() {
            try {
                audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                audioContext = new AudioContext();
                
                status.textContent = "Audio initialized";
                startBtn.disabled = false;
            } catch (error) {
                console.error("Error initializing audio:", error);
                status.textContent = "Error: Could not access microphone";
            }
        }
        
        // Start recording
        function startRecording() {
            if (!audioStream) {
                status.textContent = "Error: Audio not initialized";
                return;
            }
            
            // Clear previous transcription text
            transcriptionText = "";
            transcription.textContent = "";
            
            // Setup WebSocket
            const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsUrl = `${protocol}//${window.location.host}/asr`;
            
            socket = new WebSocket(wsUrl);
            
            socket.onopen = () => {
                status.textContent = "Connected. Listening...";
                
                // Set up media recorder
                mediaRecorder = new MediaRecorder(audioStream);
                
                // Process audio chunks
                mediaRecorder.ondataavailable = (event) => {
                    if (event.data.size > 0 && socket.readyState === WebSocket.OPEN) {
                        event.data.arrayBuffer().then(buffer => {
                            socket.send(buffer);
                        });
                    }
                };
                
                // Start recording
                mediaRecorder.start(100); // Capture in 100ms chunks
                isRecording = true;
                
                // Update UI
                startBtn.style.display = 'none';
                stopBtn.style.display = 'inline-block';
            };
            
            socket.onmessage = (event) => {
                const data = JSON.parse(event.data);
                if (data.text) {
                    // Accumulate all text chunks into one string
                    transcriptionText += " " + data.text;
                    transcription.textContent = transcriptionText.trim();
                }
            };
            
            socket.onerror = (error) => {
                console.error("WebSocket error:", error);
                status.textContent = "Error in connection";
            };
            
            socket.onclose = () => {
                if (isRecording) {
                    stopRecording();
                    status.textContent = "Connection closed";
                }
            };
        }
        
        // Stop recording
        function stopRecording() {
            if (mediaRecorder && isRecording) {
                mediaRecorder.stop();
                isRecording = false;
                
                // Update UI
                stopBtn.style.display = 'none';
                startBtn.style.display = 'inline-block';
                status.textContent = "Processing...";
                
                // Close WebSocket
                if (socket && socket.readyState === WebSocket.OPEN) {
                    socket.close();
                }
                
                // Generate response with the complete accumulated text
                const finalText = transcriptionText.trim();
                if (finalText) {
                    console.log("Final transcription:", finalText);
                    // Only now do we process the complete text and send it to TTS
                    generateResponse(finalText);
                } else {
                    status.textContent = "No speech detected";
                }
            }
        }
        
        // Generate TTS response
        function generateResponse(text) {
            // Clear previous response
            response.innerHTML = '';
            audioQueue = [];
            audioPlayer.src = '';
            isPlaying = false;
            
            // Show loading indicator
            const loadingElement = document.createElement('div');
            loadingElement.className = 'loading';
            response.appendChild(loadingElement);
            response.appendChild(document.createTextNode(' Generating response...'));
            
            // Clean up the text - important to use the accumulated text from all ASR chunks
            const cleanText = text.trim();
            console.log("Sending complete text for processing:", cleanText);
            
            // Create a new EventSource for TTS streaming
            const sse = new EventSource(`${window.location.origin}/stream-tts?query=${encodeURIComponent(cleanText)}`);
                        
            sse.addEventListener('ttsUpdate', (event) => {
                const data = JSON.parse(event.data);
                
                // Remove loading indicator if it's the first update
                if (loadingElement.parentNode) {
                    response.removeChild(loadingElement);
                }
                
                // Update response text with paragraph elements
                const paragraph = document.createElement('p');
                paragraph.textContent = data.text;
                response.appendChild(paragraph);
                
                // Process audio
                const audioBytes = hexToBytes(data.audio);
                const blob = new Blob([audioBytes], { type: 'audio/mp3' });
                const url = URL.createObjectURL(blob);
                
                // Add to queue with duration
                audioQueue.push({ url: url, duration: data.duration });
                
                // Start playing if not already playing
                playNextAudio();
            });
            
            sse.addEventListener('ttsEnd', (event) => {
                sse.close();
                status.textContent = "Ready";
                
                // Reset transcription for next recording
                transcriptionText = "";
                transcription.textContent = "";
            });
            
            sse.onerror = (error) => {
                console.error("SSE error:", error);
                sse.close();
                
                // Remove loading indicator if it still exists
                if (loadingElement.parentNode) {
                    response.removeChild(loadingElement);
                }
                
                response.textContent = "Error generating response";
                status.textContent = "Ready";
            };
        }
        
        // Play the next audio in queue
        function playNextAudio() {
            if (isPlaying || audioQueue.length === 0) return;
            
            isPlaying = true;
            const nextAudio = audioQueue.shift();
            audioPlayer.src = nextAudio.url;
            
            audioPlayer.play().then(() => {
                audioPlayer.onended = () => {
                    // Clean up and play next
                    URL.revokeObjectURL(audioPlayer.src);
                    isPlaying = false;
                    playNextAudio();
                };
            }).catch(error => {
                console.error('Audio playback failed:', error);
                isPlaying = false;
                playNextAudio();
            });
        }
        
        // Helper: Convert hex string to byte array
        function hexToBytes(hex) {
            const bytes = new Uint8Array(hex.length / 2);
            for (let i = 0; i < hex.length; i += 2) {
                bytes[i / 2] = parseInt(hex.substring(i, i + 2), 16);
            }
            return bytes;
        }
        
        // Event listeners
        startBtn.addEventListener('click', startRecording);
        stopBtn.addEventListener('click', stopRecording);
        
        // Initialize on page load
        window.addEventListener('load', initAudio);
    </script>
</body>
</html>